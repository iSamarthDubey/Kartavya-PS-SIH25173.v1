# 📁 KARTAVYA - SIEM NLP ASSISTANT

# Repository Structure & Component Documentation

================================================================================
PROJECT: Kartavya - Smart SIEM NLP Assistant
TEAM: SIH 2025
PURPOSE: Natural Language Interface for SIEM Query & Analysis
STATUS: Production Ready
================================================================================

## 🏗️ ROOT DIRECTORY STRUCTURE

Kartavya-PS-SIH25173.v1/
│
├── assistant/                  # 🤖 Conversational AI Module (Port 8001)
├── backend/                    # 🧠 Core Processing Engine
├── siem_connector/             # 🔌 SIEM Platform Integrations
├── rag_pipeline/               # 📚 RAG & LLM Intelligence
├── ui_dashboard/               # 🎨 Streamlit Web Interface (Port 8502)
├── tests/                      # 🧪 Test Suite (Unit, Integration, E2E)
├── docs/                       # 📚 Project Documentation
├── scripts/                    # 🛠️ Utility Scripts
├── datasets/                   # 📊 Training & Sample Data
├── docker/                     # 🐳 Docker Configurations
├── beats-config/               # ⚙️ Elastic Beats Configuration
├── embeddings/                 # 🔢 Vector Embeddings Storage
├── llm_training/               # 🎓 LLM Training Resources
│
├── .env                        # 🔐 Environment Variables
├── .gitignore                  # 🚫 Git Exclusions
├── requirements.txt            # 📦 Python Dependencies
├── requirements-prod.txt       # 📦 Production Dependencies
├── requirements-docker.txt     # 📦 Docker Dependencies
├── README.md                   # 📖 Main Project Documentation
├── LICENSE                     # ⚖️ License Information
└── REPOSITORY_STRUCTURE.txt    # 📋 This File

================================================================================

## 📦 MODULE DETAILS

================================================================================

┌──────────────────────────────────────────────────────────────────────────┐
│ 1. ASSISTANT MODULE (assistant/)                                         │
│    🤖 Conversational AI Orchestrator                                     │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Main entry point for the conversational assistant system. Orchestrates
    the complete NLP → SIEM → Response workflow through a FastAPI REST API.

KEY FILES:
    ├── main.py                 # FastAPI server (runs on port 8001)
    ├── router.py               # API route definitions (/assistant/ask)
    ├── pipeline.py             # Core orchestration logic
    ├── models.py               # Pydantic request/response models
    ├── context_manager.py      # Conversation state management
    ├── verify_integration.py   # Integration validation script
    └── __init__.py             # Module initialization

RESPONSIBILITIES:
    • Receive natural language queries via REST API
    • Orchestrate NLP analysis (intent + entity extraction)
    • Generate SIEM queries from natural language
    • Execute queries against Elasticsearch/Wazuh
    • Format and return structured responses
    • Manage conversation context and history

API ENDPOINTS:
    POST /assistant/ask         # Main query endpoint
    GET  /assistant/health      # Health check
    POST /assistant/clear       # Clear conversation context

DEPENDENCIES:
    backend.nlp (IntentClassifier, EntityExtractor)
    backend.query_builder (QueryBuilder)
    siem_connector (ElasticConnector, WazuhConnector)
    backend.response_formatter (ResponseFormatter)

RUN COMMAND:
    python assistant/main.py

┌──────────────────────────────────────────────────────────────────────────┐
│ 2. BACKEND MODULE (backend/)                                             │
│    🧠 Core Processing & Intelligence                                     │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Core processing components including NLP analysis, query generation,
    and response formatting. The brain of the system.

STRUCTURE:
    backend/
    ├── nlp/                    # Natural Language Processing
    │   ├── intent_classifier.py    # Classify query intent (10+ types)
    │   ├── entity_extractor.py     # Extract entities (12+ types)
    │   └── __init__.py
    ├── response_formatter/     # Output Formatting
    │   ├── formatter.py            # Main response formatter
    │   ├── text_formatter.py       # Plain text formatting
    │   ├── chart_formatter.py      # Visualization generation
    │   ├── dashboard_export.py     # Dashboard export formats
    │   └── __init__.py
    ├── query_builder.py        # NL → Elasticsearch DSL conversion
    ├── elastic_client.py       # Elasticsearch helper utilities
    ├── main.py                 # Backend server (if standalone)
    └── __init__.py

SUBMODULE: backend/nlp/
    INTENT CLASSIFIER:
        • Classifies user queries into 10+ intent categories
        • Uses pattern matching with confidence scoring
        • Intents: SEARCH_LOGS, AUTHENTICATION, NETWORK_SECURITY,
                  MALWARE_DETECTION, DATA_EXFILTRATION, etc.
        • Returns: (QueryIntent enum, confidence score)

    ENTITY EXTRACTOR:
        • Extracts 12+ entity types from queries
        • Entity Types: ip_address, username, domain, port, file_path,
                       time_range, severity, process_name, etc.
        • Uses regex patterns + validation
        • Returns: List[Entity] with confidence scores

SUBMODULE: backend/response_formatter/
    RESPONSE FORMATTER:
        • Formats raw SIEM results for UI display
        • Generates summaries and insights
        • Creates visualization configurations
        • Supports multiple output formats (JSON, text, charts)

QUERY BUILDER:
    • Converts natural language to Elasticsearch DSL
    • Intent-based query generation
    • Handles time ranges, filters, aggregations
    • Supports complex bool queries

USAGE:
    from backend.nlp import IntentClassifier, EntityExtractor
    from backend.query_builder import QueryBuilder
    from backend.response_formatter import ResponseFormatter

┌──────────────────────────────────────────────────────────────────────────┐
│ 3. SIEM CONNECTOR MODULE (siem_connector/)                               │
│    🔌 SIEM Platform Integration Layer                                    │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Provides connectors for integrating with various SIEM platforms.
    Handles authentication, query execution, and data retrieval.

KEY FILES:
    ├── elastic_connector.py    # Elasticsearch integration
    ├── wazuh_connector.py      # Wazuh SIEM integration
    ├── utils.py                # Common utilities
    └── __init__.py

ELASTICSEARCH CONNECTOR (elastic_connector.py):
    • Async connection to Elasticsearch
    • Query execution with pagination
    • Health checks and monitoring
    • Error handling and retry logic
    • Index management

    Configuration:
        ELASTICSEARCH_HOST=localhost
        ELASTICSEARCH_PORT=9200
        ELASTICSEARCH_USER=elastic
        ELASTICSEARCH_PASSWORD=changeme

WAZUH CONNECTOR (wazuh_connector.py):
    • Wazuh API integration
    • Alert retrieval and analysis
    • Agent management
    • Rule querying

    Configuration:
        WAZUH_HOST=localhost
        WAZUH_PORT=55000
        WAZUH_USER=wazuh
        WAZUH_PASSWORD=wazuh

UTILITIES (utils.py):
    • Log normalization
    • Field extraction
    • Query validation
    • Response sanitization

USAGE:
    from siem_connector import ElasticConnector, WazuhConnector

    connector = ElasticConnector()
    results = await connector.search(query="...", limit=100)

┌──────────────────────────────────────────────────────────────────────────┐
│ 4. RAG PIPELINE MODULE (rag_pipeline/)                                   │
│    📚 Retrieval-Augmented Generation (Future Enhancement)                │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    RAG (Retrieval-Augmented Generation) pipeline for intelligent
    document retrieval and LLM-powered response generation.
    __CURRENTLY STUB - READY FOR IMPLEMENTATION__

STRUCTURE:
    ├── pipeline.py             # Main RAG orchestrator
    ├── retriever.py            # Document retrieval
    ├── vector_store.py         # Vector database interface
    ├── prompt_builder.py       # LLM prompt construction
    └── __init__.py

PLANNED FEATURES:
    • Vector similarity search for log analysis
    • LLM-based threat intelligence summarization
    • Context-aware query expansion
    • Intelligent alert correlation
    • Natural language report generation

FUTURE USAGE:
    from rag_pipeline import RAGPipeline

    rag = RAGPipeline()
    enhanced_response = await rag.enhance_response(query, results)

┌──────────────────────────────────────────────────────────────────────────┐
│ 5. UI DASHBOARD MODULE (ui_dashboard/)                                   │
│    🎨 Streamlit Web Interface (Port 8502)                                │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Modern web interface for interacting with the SIEM NLP Assistant.
    Provides chat interface, visualizations, and result exploration.

KEY FILES:
    ├── streamlit_app.py        # Main Streamlit application
    ├── demo_data.py            # Mock data generator
    ├── static/                 # Static assets (CSS, JS, images)
    └── templates/              # HTML templates

FEATURES:
    • Chat-based query interface
    • Real-time query processing
    • Entity highlighting in queries
    • Interactive result tables
    • Chart and graph visualizations
    • Query history and favorites
    • Export results (CSV, JSON)
    • Dashboard overview

PAGES:
    1. Chat Interface          # Main query page
    2. Dashboard Overview      # Metrics and stats
    3. Alert Analysis          # Alert deep dive
    4. Query History           # Past queries
    5. Settings                # Configuration

RUN COMMAND:
    streamlit run ui_dashboard/streamlit_app.py

ACCESS URL:
    <http://localhost:8502>

┌──────────────────────────────────────────────────────────┐
│ 6. TESTS MODULE (tests/)                                 │
│   Comprehensive Test Suite                               │
└──────────────────────────────────────────────────────────┘

PURPOSE:
    Organized testing infrastructure covering unit tests, integration
    tests, and end-to-end testing scenarios.

STRUCTURE:
    tests/
    ├── unit/                   # Unit tests for individual components
    │   ├── test_intent_classifier.py
    │   ├── test_entity_extractor.py
    │   ├── test_query_builder.py
    │   └── test_formatters.py
    ├── integration/            # Integration tests
    │   ├── test_pipeline.py
    │   ├── test_connectors.py
    │   └── test_api.py
    ├── e2e/                    # End-to-end tests
    │   └── test_complete_integration.py
    ├── test_connector.py       # SIEM connector tests
    └── test_parser.py          # Parser tests

TEST COVERAGE:
    • NLP components (intent, entity extraction)
    • Query generation
    • SIEM connectors
    • Response formatting
    • Complete pipeline flow
    • API endpoints

RUN COMMANDS:
    pytest tests/                           # All tests
    pytest tests/unit/                      # Unit tests only
    pytest tests/integration/               # Integration tests
    python tests/e2e/test_complete_integration.py  # E2E tests

┌──────────────────────────────────────────────────────────────────────┐
│ 7. DOCS MODULE (docs/)                                               │
│    Project Documentation                                             │
└──────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Consolidated project documentation including architecture,
    integration status, and development guides.

FILES:
    ├── BACKEND_INTEGRATION_COMPLETE.md    # Integration status
    ├── INTEGRATION_STATUS_FINAL.md        # Final status report
    ├── CLEANUP_PLAN.md                    # Cleanup documentation
    ├── CLEANUP_COMPLETE.md                # Cleanup results
    ├── RESTRUCTURING_COMPLETE.md          # Restructuring docs
    └── VERSION_STRATEGY_GUIDE.md          # Version control guide

NOTE:
    These files are excluded from Git (only README.md is tracked).
    They are maintained locally for development reference.

┌───────────────────────────────────────────────────────────────────────┐
│ 8. SCRIPTS MODULE (scripts/)                                          │
│    Utility Scripts & Tools                                            │
└───────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Helper scripts for data ingestion, setup, and maintenance.

KEY FILES:
    ├── ingest_logs.py              # Ingest sample logs to Elasticsearch
    ├── setup_beats.ps1             # Setup Elastic Beats (PowerShell)
    ├── update_dependencies.ps1     # Update Python dependencies (PS)
    └── update_dependencies.sh      # Update Python dependencies (Bash)

USAGE:
    # Ingest sample logs
    python scripts/ingest_logs.py --source datasets/raw/

    # Setup Beats
    .\scripts\setup_beats.ps1

    # Update dependencies
    .\scripts\update_dependencies.ps1

┌──────────────────────────────────────────────────────────────────────────┐
│ 9. DATASETS MODULE (datasets/)                                           │
│    📊 Training & Sample Data                                             │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Storage for training data, sample logs, and synthetic datasets.

STRUCTURE:
    datasets/
    ├── raw/                    # Raw log files
    │   ├── ctu13/             # CTU-13 malware dataset
    │   ├── loghub/            # LogHub datasets
    │   ├── splunk_attack/     # Splunk security samples
    │   └── wazuh_samples/     # Wazuh sample logs
    ├── processed/             # Cleaned/processed data
    └── synthetic/             # Synthetically generated logs

DATASETS:
    • CTU-13: Botnet traffic captures
    • LogHub: Various system log formats
    • Splunk Attack Data: Security event samples
    • Wazuh Samples: SIEM alert examples

┌──────────────────────────────────────────────────────────────────────────┐
│ 10. DOCKER MODULE (docker/)                                              │
│     🐳 Docker Configurations                                             │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Docker configurations for containerized deployment.

STRUCTURE:
    docker/
    ├── docker-compose.yml      # Main compose file
    └── logstash/               # Logstash configuration
        ├── config/
        └── pipeline/

SERVICES:
    • Elasticsearch (port 9200)
    • Kibana (port 5601)
    • Logstash (port 5044)
    • Assistant API (port 8001)
    • Streamlit UI (port 8502)

RUN COMMAND:
    docker-compose -f docker/docker-compose.yml up -d

┌──────────────────────────────────────────────────────────────────────────┐
│ 11. BEATS CONFIG MODULE (beats-config/)                                  │
│     ⚙️ Elastic Beats Configuration Files                                 │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Configuration files for Elastic Beats data shippers.

FILES:
    ├── metricbeat.yml          # System metrics collection
    └── winlogbeat.yml          # Windows event log collection

USAGE:
    Configure these files with your Elasticsearch endpoint,
    then deploy using Elastic Beats agents.

┌──────────────────────────────────────────────────────────────────────────┐
│ 12. EMBEDDINGS MODULE (embeddings/)                                      │
│     🔢 Vector Embeddings Storage                                         │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Storage for vector embeddings used in semantic search and RAG.

STRUCTURE:
    embeddings/
    ├── elastic_vectors/        # Elasticsearch vector storage
    └── faiss/                  # FAISS index files

NOTE:
    Currently prepared for future RAG implementation.

┌──────────────────────────────────────────────────────────────────────────┐
│ 13. LLM TRAINING MODULE (llm_training/)                                  │
│     🎓 LLM Training Resources                                            │
└──────────────────────────────────────────────────────────────────────────┘

PURPOSE:
    Training scripts and datasets for fine-tuning LLM models.

FILES:
    ├── train.py                # Training script
    ├── evaluate.py             # Model evaluation
    ├── nl_to_query_pairs.json  # Training pairs (NL → Query)
    └── tokenizer/              # Custom tokenizer configs

USAGE:
    python llm_training/train.py --dataset nl_to_query_pairs.json

================================================================================

## 🔄 DATA FLOW

================================================================================

USER QUERY → WORKFLOW:

1. USER INPUT
   └─→ Streamlit UI (ui_dashboard/streamlit_app.py)
       └─→ HTTP POST to <http://localhost:8001/assistant/ask>

2. API GATEWAY
   └─→ FastAPI Router (assistant/router.py)
       └─→ ConversationalPipeline (assistant/pipeline.py)

3. NLP ANALYSIS
   └─→ IntentClassifier (backend/nlp/intent_classifier.py)
       • Determines query intent (10+ types)
       • Confidence scoring
   └─→ EntityExtractor (backend/nlp/entity_extractor.py)
       • Extracts entities (12+ types)
       • IP addresses, usernames, time ranges, etc.

4. QUERY GENERATION
   └─→ QueryBuilder (backend/query_builder.py)
       • Converts NL → Elasticsearch DSL
       • Builds bool queries with filters

5. SIEM EXECUTION
   └─→ ElasticConnector (siem_connector/elastic_connector.py)
       • Execute query against Elasticsearch
       • Retrieve matching logs/alerts
   └─→ WazuhConnector (siem_connector/wazuh_connector.py)
       • Query Wazuh API if needed
       • Aggregate results

6. RESPONSE FORMATTING
   └─→ ResponseFormatter (backend/response_formatter/formatter.py)
       • Format raw results
       • Generate summaries
       • Create visualizations

7. CONTEXT MANAGEMENT
   └─→ ContextManager (assistant/context_manager.py)
       • Save conversation history
       • Update user context

8. RESPONSE DELIVERY
   └─→ FastAPI Response → Streamlit UI
       └─→ Display results to user

================================================================================

## 🚀 QUICK START COMMANDS

================================================================================

1. SETUP ENVIRONMENT:
   python -m venv venv
   .\venv\Scripts\activate          # Windows
   pip install -r requirements.txt

2. START BACKEND:
   python assistant/main.py

   # Runs on <http://localhost:8001>

3. START FRONTEND:
   streamlit run ui_dashboard/streamlit_app.py

   # Opens browser at <http://localhost:8502>

4. RUN TESTS:
   pytest tests/
   python tests/e2e/test_complete_integration.py

5. DOCKER DEPLOYMENT:
   docker-compose -f docker/docker-compose.yml up -d

================================================================================

## 📝 CONFIGURATION FILES

================================================================================

.env                            # Environment variables
    ELASTICSEARCH_HOST=localhost
    ELASTICSEARCH_PORT=9200
    WAZUH_HOST=localhost
    WAZUH_PORT=55000
    API_PORT=8001
    DEBUG=True

requirements.txt                # All Python dependencies
requirements-prod.txt           # Production-only dependencies
requirements-docker.txt         # Docker image dependencies

.gitignore                      # Git exclusions
    • Ignores ALL .md files except README.md
    • Excludes __pycache__, *.pyc, .env
    • Excludes test databases, logs

================================================================================

## 🎯 COMPONENT STATUS

================================================================================

✅ PRODUCTION READY:
    • assistant/                (FastAPI server + pipeline)
    • backend/nlp/             (Intent + Entity extraction)
    • backend/query_builder.py (NL → Elasticsearch DSL)
    • siem_connector/          (Elasticsearch & Wazuh connectors)
    • backend/response_formatter/ (Response formatting)
    • ui_dashboard/            (Streamlit interface)
    • tests/                   (Comprehensive test suite)

🚧 READY FOR IMPLEMENTATION:
    • rag_pipeline/            (RAG & LLM intelligence)
    • llm_training/            (Model fine-tuning)

⚠️  REQUIRES EXTERNAL SERVICES:
    • Elasticsearch (port 9200) - For SIEM data storage
    • Wazuh (port 55000) - Optional additional SIEM

================================================================================

## 📊 STATISTICS

================================================================================

Total Python Files:     50+
Total Lines of Code:    15,000+
Test Coverage:          Core components tested
API Endpoints:          3 (ask, health, clear)
NLP Intent Types:       10+
Entity Types:           12+
Supported SIEMs:        Elasticsearch, Wazuh

================================================================================

## 👥 TEAM & SUPPORT

================================================================================

Project:        Kartavya - SIEM NLP Assistant
Team:           SIH 2025
Repository:     github.com/iSamarthDubey/Kartavya-PS-SIH25173.v1
Documentation:  See README.md for detailed setup
Issues:         Use GitHub Issues for bug reports

================================================================================

## 📄 LICENSE

================================================================================

See LICENSE file for licensing information.

================================================================================
END OF REPOSITORY STRUCTURE DOCUMENTATION
Generated: October 4, 2025
Version: 1.0
Status: ✅ PRODUCTION READY
================================================================================
