# ğŸ“ KARTAVYA - SIEM NLP ASSISTANT

# Repository Structure & Component Documentation

================================================================================
PROJECT: Kartavya - Smart SIEM NLP Assistant
TEAM: SIH 2025
PURPOSE: Natural Language Interface for SIEM Query & Analysis
STATUS: Production Ready
================================================================================

## ğŸ—ï¸ ROOT DIRECTORY STRUCTURE

Kartavya-PS-SIH25173.v1/
â”‚
â”œâ”€â”€ assistant/                  # ğŸ¤– Conversational AI Module (Port 8001)
â”œâ”€â”€ backend/                    # ğŸ§  Core Processing Engine
â”œâ”€â”€ siem_connector/             # ğŸ”Œ SIEM Platform Integrations
â”œâ”€â”€ rag_pipeline/               # ğŸ“š RAG & LLM Intelligence
â”œâ”€â”€ ui_dashboard/               # ğŸ¨ Streamlit Web Interface (Port 8502)
â”œâ”€â”€ tests/                      # ğŸ§ª Test Suite (Unit, Integration, E2E)
â”œâ”€â”€ docs/                       # ğŸ“š Project Documentation
â”œâ”€â”€ scripts/                    # ğŸ› ï¸ Utility Scripts
â”œâ”€â”€ datasets/                   # ğŸ“Š Training & Sample Data
â”œâ”€â”€ docker/                     # ğŸ³ Docker Configurations
â”œâ”€â”€ beats-config/               # âš™ï¸ Elastic Beats Configuration
â”œâ”€â”€ embeddings/                 # ğŸ”¢ Vector Embeddings Storage
â”œâ”€â”€ llm_training/               # ğŸ“ LLM Training Resources
â”‚
â”œâ”€â”€ .env                        # ğŸ” Environment Variables
â”œâ”€â”€ .gitignore                  # ğŸš« Git Exclusions
â”œâ”€â”€ requirements.txt            # ğŸ“¦ Python Dependencies
â”œâ”€â”€ requirements-prod.txt       # ğŸ“¦ Production Dependencies
â”œâ”€â”€ requirements-docker.txt     # ğŸ“¦ Docker Dependencies
â”œâ”€â”€ README.md                   # ğŸ“– Main Project Documentation
â”œâ”€â”€ LICENSE                     # âš–ï¸ License Information
â””â”€â”€ REPOSITORY_STRUCTURE.txt    # ğŸ“‹ This File

================================================================================

## ğŸ“¦ MODULE DETAILS

================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. ASSISTANT MODULE (assistant/)                                         â”‚
â”‚    ğŸ¤– Conversational AI Orchestrator                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Main entry point for the conversational assistant system. Orchestrates
    the complete NLP â†’ SIEM â†’ Response workflow through a FastAPI REST API.

KEY FILES:
    â”œâ”€â”€ main.py                 # FastAPI server (runs on port 8001)
    â”œâ”€â”€ router.py               # API route definitions (/assistant/ask)
    â”œâ”€â”€ pipeline.py             # Core orchestration logic
    â”œâ”€â”€ models.py               # Pydantic request/response models
    â”œâ”€â”€ context_manager.py      # Conversation state management
    â”œâ”€â”€ verify_integration.py   # Integration validation script
    â””â”€â”€ __init__.py             # Module initialization

RESPONSIBILITIES:
    â€¢ Receive natural language queries via REST API
    â€¢ Orchestrate NLP analysis (intent + entity extraction)
    â€¢ Generate SIEM queries from natural language
    â€¢ Execute queries against Elasticsearch/Wazuh
    â€¢ Format and return structured responses
    â€¢ Manage conversation context and history

API ENDPOINTS:
    POST /assistant/ask         # Main query endpoint
    GET  /assistant/health      # Health check
    POST /assistant/clear       # Clear conversation context

DEPENDENCIES:
    backend.nlp (IntentClassifier, EntityExtractor)
    backend.query_builder (QueryBuilder)
    siem_connector (ElasticConnector, WazuhConnector)
    backend.response_formatter (ResponseFormatter)

RUN COMMAND:
    python assistant/main.py

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. BACKEND MODULE (backend/)                                             â”‚
â”‚    ğŸ§  Core Processing & Intelligence                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Core processing components including NLP analysis, query generation,
    and response formatting. The brain of the system.

STRUCTURE:
    backend/
    â”œâ”€â”€ nlp/                    # Natural Language Processing
    â”‚   â”œâ”€â”€ intent_classifier.py    # Classify query intent (10+ types)
    â”‚   â”œâ”€â”€ entity_extractor.py     # Extract entities (12+ types)
    â”‚   â””â”€â”€ __init__.py
    â”œâ”€â”€ response_formatter/     # Output Formatting
    â”‚   â”œâ”€â”€ formatter.py            # Main response formatter
    â”‚   â”œâ”€â”€ text_formatter.py       # Plain text formatting
    â”‚   â”œâ”€â”€ chart_formatter.py      # Visualization generation
    â”‚   â”œâ”€â”€ dashboard_export.py     # Dashboard export formats
    â”‚   â””â”€â”€ __init__.py
    â”œâ”€â”€ query_builder.py        # NL â†’ Elasticsearch DSL conversion
    â”œâ”€â”€ elastic_client.py       # Elasticsearch helper utilities
    â”œâ”€â”€ main.py                 # Backend server (if standalone)
    â””â”€â”€ __init__.py

SUBMODULE: backend/nlp/
    INTENT CLASSIFIER:
        â€¢ Classifies user queries into 10+ intent categories
        â€¢ Uses pattern matching with confidence scoring
        â€¢ Intents: SEARCH_LOGS, AUTHENTICATION, NETWORK_SECURITY,
                  MALWARE_DETECTION, DATA_EXFILTRATION, etc.
        â€¢ Returns: (QueryIntent enum, confidence score)

    ENTITY EXTRACTOR:
        â€¢ Extracts 12+ entity types from queries
        â€¢ Entity Types: ip_address, username, domain, port, file_path,
                       time_range, severity, process_name, etc.
        â€¢ Uses regex patterns + validation
        â€¢ Returns: List[Entity] with confidence scores

SUBMODULE: backend/response_formatter/
    RESPONSE FORMATTER:
        â€¢ Formats raw SIEM results for UI display
        â€¢ Generates summaries and insights
        â€¢ Creates visualization configurations
        â€¢ Supports multiple output formats (JSON, text, charts)

QUERY BUILDER:
    â€¢ Converts natural language to Elasticsearch DSL
    â€¢ Intent-based query generation
    â€¢ Handles time ranges, filters, aggregations
    â€¢ Supports complex bool queries

USAGE:
    from backend.nlp import IntentClassifier, EntityExtractor
    from backend.query_builder import QueryBuilder
    from backend.response_formatter import ResponseFormatter

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SIEM CONNECTOR MODULE (siem_connector/)                               â”‚
â”‚    ğŸ”Œ SIEM Platform Integration Layer                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Provides connectors for integrating with various SIEM platforms.
    Handles authentication, query execution, and data retrieval.

KEY FILES:
    â”œâ”€â”€ elastic_connector.py    # Elasticsearch integration
    â”œâ”€â”€ wazuh_connector.py      # Wazuh SIEM integration
    â”œâ”€â”€ utils.py                # Common utilities
    â””â”€â”€ __init__.py

ELASTICSEARCH CONNECTOR (elastic_connector.py):
    â€¢ Async connection to Elasticsearch
    â€¢ Query execution with pagination
    â€¢ Health checks and monitoring
    â€¢ Error handling and retry logic
    â€¢ Index management

    Configuration:
        ELASTICSEARCH_HOST=localhost
        ELASTICSEARCH_PORT=9200
        ELASTICSEARCH_USER=elastic
        ELASTICSEARCH_PASSWORD=changeme

WAZUH CONNECTOR (wazuh_connector.py):
    â€¢ Wazuh API integration
    â€¢ Alert retrieval and analysis
    â€¢ Agent management
    â€¢ Rule querying

    Configuration:
        WAZUH_HOST=localhost
        WAZUH_PORT=55000
        WAZUH_USER=wazuh
        WAZUH_PASSWORD=wazuh

UTILITIES (utils.py):
    â€¢ Log normalization
    â€¢ Field extraction
    â€¢ Query validation
    â€¢ Response sanitization

USAGE:
    from siem_connector import ElasticConnector, WazuhConnector

    connector = ElasticConnector()
    results = await connector.search(query="...", limit=100)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. RAG PIPELINE MODULE (rag_pipeline/)                                   â”‚
â”‚    ğŸ“š Retrieval-Augmented Generation (Future Enhancement)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    RAG (Retrieval-Augmented Generation) pipeline for intelligent
    document retrieval and LLM-powered response generation.
    __CURRENTLY STUB - READY FOR IMPLEMENTATION__

STRUCTURE:
    â”œâ”€â”€ pipeline.py             # Main RAG orchestrator
    â”œâ”€â”€ retriever.py            # Document retrieval
    â”œâ”€â”€ vector_store.py         # Vector database interface
    â”œâ”€â”€ prompt_builder.py       # LLM prompt construction
    â””â”€â”€ __init__.py

PLANNED FEATURES:
    â€¢ Vector similarity search for log analysis
    â€¢ LLM-based threat intelligence summarization
    â€¢ Context-aware query expansion
    â€¢ Intelligent alert correlation
    â€¢ Natural language report generation

FUTURE USAGE:
    from rag_pipeline import RAGPipeline

    rag = RAGPipeline()
    enhanced_response = await rag.enhance_response(query, results)

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. UI DASHBOARD MODULE (ui_dashboard/)                                   â”‚
â”‚    ğŸ¨ Streamlit Web Interface (Port 8502)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Modern web interface for interacting with the SIEM NLP Assistant.
    Provides chat interface, visualizations, and result exploration.

KEY FILES:
    â”œâ”€â”€ streamlit_app.py        # Main Streamlit application
    â”œâ”€â”€ demo_data.py            # Mock data generator
    â”œâ”€â”€ static/                 # Static assets (CSS, JS, images)
    â””â”€â”€ templates/              # HTML templates

FEATURES:
    â€¢ Chat-based query interface
    â€¢ Real-time query processing
    â€¢ Entity highlighting in queries
    â€¢ Interactive result tables
    â€¢ Chart and graph visualizations
    â€¢ Query history and favorites
    â€¢ Export results (CSV, JSON)
    â€¢ Dashboard overview

PAGES:
    1. Chat Interface          # Main query page
    2. Dashboard Overview      # Metrics and stats
    3. Alert Analysis          # Alert deep dive
    4. Query History           # Past queries
    5. Settings                # Configuration

RUN COMMAND:
    streamlit run ui_dashboard/streamlit_app.py

ACCESS URL:
    <http://localhost:8502>

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. TESTS MODULE (tests/)                                 â”‚
â”‚   Comprehensive Test Suite                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Organized testing infrastructure covering unit tests, integration
    tests, and end-to-end testing scenarios.

STRUCTURE:
    tests/
    â”œâ”€â”€ unit/                   # Unit tests for individual components
    â”‚   â”œâ”€â”€ test_intent_classifier.py
    â”‚   â”œâ”€â”€ test_entity_extractor.py
    â”‚   â”œâ”€â”€ test_query_builder.py
    â”‚   â””â”€â”€ test_formatters.py
    â”œâ”€â”€ integration/            # Integration tests
    â”‚   â”œâ”€â”€ test_pipeline.py
    â”‚   â”œâ”€â”€ test_connectors.py
    â”‚   â””â”€â”€ test_api.py
    â”œâ”€â”€ e2e/                    # End-to-end tests
    â”‚   â””â”€â”€ test_complete_integration.py
    â”œâ”€â”€ test_connector.py       # SIEM connector tests
    â””â”€â”€ test_parser.py          # Parser tests

TEST COVERAGE:
    â€¢ NLP components (intent, entity extraction)
    â€¢ Query generation
    â€¢ SIEM connectors
    â€¢ Response formatting
    â€¢ Complete pipeline flow
    â€¢ API endpoints

RUN COMMANDS:
    pytest tests/                           # All tests
    pytest tests/unit/                      # Unit tests only
    pytest tests/integration/               # Integration tests
    python tests/e2e/test_complete_integration.py  # E2E tests

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. DOCS MODULE (docs/)                                               â”‚
â”‚    Project Documentation                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Consolidated project documentation including architecture,
    integration status, and development guides.

FILES:
    â”œâ”€â”€ BACKEND_INTEGRATION_COMPLETE.md    # Integration status
    â”œâ”€â”€ INTEGRATION_STATUS_FINAL.md        # Final status report
    â”œâ”€â”€ CLEANUP_PLAN.md                    # Cleanup documentation
    â”œâ”€â”€ CLEANUP_COMPLETE.md                # Cleanup results
    â”œâ”€â”€ RESTRUCTURING_COMPLETE.md          # Restructuring docs
    â””â”€â”€ VERSION_STRATEGY_GUIDE.md          # Version control guide

NOTE:
    These files are excluded from Git (only README.md is tracked).
    They are maintained locally for development reference.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. SCRIPTS MODULE (scripts/)                                          â”‚
â”‚    Utility Scripts & Tools                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Helper scripts for data ingestion, setup, and maintenance.

KEY FILES:
    â”œâ”€â”€ ingest_logs.py              # Ingest sample logs to Elasticsearch
    â”œâ”€â”€ setup_beats.ps1             # Setup Elastic Beats (PowerShell)
    â”œâ”€â”€ update_dependencies.ps1     # Update Python dependencies (PS)
    â””â”€â”€ update_dependencies.sh      # Update Python dependencies (Bash)

USAGE:
    # Ingest sample logs
    python scripts/ingest_logs.py --source datasets/raw/

    # Setup Beats
    .\scripts\setup_beats.ps1

    # Update dependencies
    .\scripts\update_dependencies.ps1

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 9. DATASETS MODULE (datasets/)                                           â”‚
â”‚    ğŸ“Š Training & Sample Data                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Storage for training data, sample logs, and synthetic datasets.

STRUCTURE:
    datasets/
    â”œâ”€â”€ raw/                    # Raw log files
    â”‚   â”œâ”€â”€ ctu13/             # CTU-13 malware dataset
    â”‚   â”œâ”€â”€ loghub/            # LogHub datasets
    â”‚   â”œâ”€â”€ splunk_attack/     # Splunk security samples
    â”‚   â””â”€â”€ wazuh_samples/     # Wazuh sample logs
    â”œâ”€â”€ processed/             # Cleaned/processed data
    â””â”€â”€ synthetic/             # Synthetically generated logs

DATASETS:
    â€¢ CTU-13: Botnet traffic captures
    â€¢ LogHub: Various system log formats
    â€¢ Splunk Attack Data: Security event samples
    â€¢ Wazuh Samples: SIEM alert examples

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 10. DOCKER MODULE (docker/)                                              â”‚
â”‚     ğŸ³ Docker Configurations                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Docker configurations for containerized deployment.

STRUCTURE:
    docker/
    â”œâ”€â”€ docker-compose.yml      # Main compose file
    â””â”€â”€ logstash/               # Logstash configuration
        â”œâ”€â”€ config/
        â””â”€â”€ pipeline/

SERVICES:
    â€¢ Elasticsearch (port 9200)
    â€¢ Kibana (port 5601)
    â€¢ Logstash (port 5044)
    â€¢ Assistant API (port 8001)
    â€¢ Streamlit UI (port 8502)

RUN COMMAND:
    docker-compose -f docker/docker-compose.yml up -d

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 11. BEATS CONFIG MODULE (beats-config/)                                  â”‚
â”‚     âš™ï¸ Elastic Beats Configuration Files                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Configuration files for Elastic Beats data shippers.

FILES:
    â”œâ”€â”€ metricbeat.yml          # System metrics collection
    â””â”€â”€ winlogbeat.yml          # Windows event log collection

USAGE:
    Configure these files with your Elasticsearch endpoint,
    then deploy using Elastic Beats agents.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 12. EMBEDDINGS MODULE (embeddings/)                                      â”‚
â”‚     ğŸ”¢ Vector Embeddings Storage                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Storage for vector embeddings used in semantic search and RAG.

STRUCTURE:
    embeddings/
    â”œâ”€â”€ elastic_vectors/        # Elasticsearch vector storage
    â””â”€â”€ faiss/                  # FAISS index files

NOTE:
    Currently prepared for future RAG implementation.

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 13. LLM TRAINING MODULE (llm_training/)                                  â”‚
â”‚     ğŸ“ LLM Training Resources                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

PURPOSE:
    Training scripts and datasets for fine-tuning LLM models.

FILES:
    â”œâ”€â”€ train.py                # Training script
    â”œâ”€â”€ evaluate.py             # Model evaluation
    â”œâ”€â”€ nl_to_query_pairs.json  # Training pairs (NL â†’ Query)
    â””â”€â”€ tokenizer/              # Custom tokenizer configs

USAGE:
    python llm_training/train.py --dataset nl_to_query_pairs.json

================================================================================

## ğŸ”„ DATA FLOW

================================================================================

USER QUERY â†’ WORKFLOW:

1. USER INPUT
   â””â”€â†’ Streamlit UI (ui_dashboard/streamlit_app.py)
       â””â”€â†’ HTTP POST to <http://localhost:8001/assistant/ask>

2. API GATEWAY
   â””â”€â†’ FastAPI Router (assistant/router.py)
       â””â”€â†’ ConversationalPipeline (assistant/pipeline.py)

3. NLP ANALYSIS
   â””â”€â†’ IntentClassifier (backend/nlp/intent_classifier.py)
       â€¢ Determines query intent (10+ types)
       â€¢ Confidence scoring
   â””â”€â†’ EntityExtractor (backend/nlp/entity_extractor.py)
       â€¢ Extracts entities (12+ types)
       â€¢ IP addresses, usernames, time ranges, etc.

4. QUERY GENERATION
   â””â”€â†’ QueryBuilder (backend/query_builder.py)
       â€¢ Converts NL â†’ Elasticsearch DSL
       â€¢ Builds bool queries with filters

5. SIEM EXECUTION
   â””â”€â†’ ElasticConnector (siem_connector/elastic_connector.py)
       â€¢ Execute query against Elasticsearch
       â€¢ Retrieve matching logs/alerts
   â””â”€â†’ WazuhConnector (siem_connector/wazuh_connector.py)
       â€¢ Query Wazuh API if needed
       â€¢ Aggregate results

6. RESPONSE FORMATTING
   â””â”€â†’ ResponseFormatter (backend/response_formatter/formatter.py)
       â€¢ Format raw results
       â€¢ Generate summaries
       â€¢ Create visualizations

7. CONTEXT MANAGEMENT
   â””â”€â†’ ContextManager (assistant/context_manager.py)
       â€¢ Save conversation history
       â€¢ Update user context

8. RESPONSE DELIVERY
   â””â”€â†’ FastAPI Response â†’ Streamlit UI
       â””â”€â†’ Display results to user

================================================================================

## ğŸš€ QUICK START COMMANDS

================================================================================

1. SETUP ENVIRONMENT:
   python -m venv venv
   .\venv\Scripts\activate          # Windows
   pip install -r requirements.txt

2. START BACKEND:
   python assistant/main.py

   # Runs on <http://localhost:8001>

3. START FRONTEND:
   streamlit run ui_dashboard/streamlit_app.py

   # Opens browser at <http://localhost:8502>

4. RUN TESTS:
   pytest tests/
   python tests/e2e/test_complete_integration.py

5. DOCKER DEPLOYMENT:
   docker-compose -f docker/docker-compose.yml up -d

================================================================================

## ğŸ“ CONFIGURATION FILES

================================================================================

.env                            # Environment variables
    ELASTICSEARCH_HOST=localhost
    ELASTICSEARCH_PORT=9200
    WAZUH_HOST=localhost
    WAZUH_PORT=55000
    API_PORT=8001
    DEBUG=True

requirements.txt                # All Python dependencies
requirements-prod.txt           # Production-only dependencies
requirements-docker.txt         # Docker image dependencies

.gitignore                      # Git exclusions
    â€¢ Ignores ALL .md files except README.md
    â€¢ Excludes __pycache__, *.pyc, .env
    â€¢ Excludes test databases, logs

================================================================================

## ğŸ¯ COMPONENT STATUS

================================================================================

âœ… PRODUCTION READY:
    â€¢ assistant/                (FastAPI server + pipeline)
    â€¢ backend/nlp/             (Intent + Entity extraction)
    â€¢ backend/query_builder.py (NL â†’ Elasticsearch DSL)
    â€¢ siem_connector/          (Elasticsearch & Wazuh connectors)
    â€¢ backend/response_formatter/ (Response formatting)
    â€¢ ui_dashboard/            (Streamlit interface)
    â€¢ tests/                   (Comprehensive test suite)

ğŸš§ READY FOR IMPLEMENTATION:
    â€¢ rag_pipeline/            (RAG & LLM intelligence)
    â€¢ llm_training/            (Model fine-tuning)

âš ï¸  REQUIRES EXTERNAL SERVICES:
    â€¢ Elasticsearch (port 9200) - For SIEM data storage
    â€¢ Wazuh (port 55000) - Optional additional SIEM

================================================================================

## ğŸ“Š STATISTICS

================================================================================

Total Python Files:     50+
Total Lines of Code:    15,000+
Test Coverage:          Core components tested
API Endpoints:          3 (ask, health, clear)
NLP Intent Types:       10+
Entity Types:           12+
Supported SIEMs:        Elasticsearch, Wazuh

================================================================================

## ğŸ‘¥ TEAM & SUPPORT

================================================================================

Project:        Kartavya - SIEM NLP Assistant
Team:           SIH 2025
Repository:     github.com/iSamarthDubey/Kartavya-PS-SIH25173.v1
Documentation:  See README.md for detailed setup
Issues:         Use GitHub Issues for bug reports

================================================================================

## ğŸ“„ LICENSE

================================================================================

See LICENSE file for licensing information.

================================================================================
END OF REPOSITORY STRUCTURE DOCUMENTATION
Generated: October 4, 2025
Version: 1.0
Status: âœ… PRODUCTION READY
================================================================================
